#+TITLE: Benchmarking OpenStack with EnOS
#+SUBTITLE: Grid'5000-FIT School 2018
#+AUTHOR: Ronan-Alexandre Cherrueau, Dimitri Pertin, Matthieu Simonin, Jonathan Pastor, Didier Iscovery
#+EMAIL: {firstname.lastname}@inria.fr
#+DATE: <2018-04-04>

#+LANGUAGE: en
#+OPTIONS: ':t
#+OPTIONS: email:t
#+OPTIONS: toc:nil

#+MACRO: eg /e.g./,
#+MACRO: ie /i.e./,
#+MACRO: i18n /$1/ (en anglais, $2)

#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="timeline.css"/>

#+BEGIN_abstract
OpenStack[fn:openstack] has become the de-facto solution to operate compute,
network and storage resources in public and private Clouds. However, developers
and scientists are facing challenges when it comes to deploy and benchmark such
a complex software stack. This lab aims at exploring
EnOS[fn:enos-paper][fn:enos-code], a holistic framework to conduct evaluations
of different OpenStack configurations in an easy and reproducible manner. In
particular, EnOS helps you in deploying real OpenStack instances on different
types of infrastructure (from virtual environments based on VMs like
Vagrant[fn:vagrant], to real large-scale testbeds composed of bare-metal
machines like Grid'5000[fn:g5k]), stressing it and getting feedback. This lab is
composed of two part:

The first part is about getting started with EnOS. More precisely we are going to:
- Deploy and configure OpenStack on Grid'5000 using EnOS;
- Operate this OpenStack to manage IaaS resources (e.g. boot VMs);
- Understand the benchmark mechanisms and run some evaluations;
- Visualize the collected metrics through Grafana.

For those who desire to go further, we propose to use EnOS to investigate
OpenStack in WAN networks. In this investigation we will study the impact of
a specific feature used in such context, just like a developer would do.
To that end, we will:
- Simulate a WAN-wide topology with EnOS by playing with traffic shaping;
- See how EnOS can be used to customize OpenStack (enable/disable features);
- Benchmark the deployed OpenStack and backup metrics;
- Analyze the collected metrics to highlight the impact of th feature.

#+END_abstract

#+TOC: headlines 3

* Table of Contents                                          :TOC@3:noexport:
- [[#requirements][Requirements]]
- [[#presentation][Presentation]]
  - [[#openstack][OpenStack]]
  - [[#enos][EnOS]]
  - [[#topology-deployed-in-this-lab][Topology deployed in this lab]]
- [[#set-the-enos-node-and-install-enos][Set the EnOS-node and install EnOS]]
- [[#deploy-openstack-using-enos][Deploy OpenStack using EnOS]]
  - [[#the-enos-configuration-file][The EnOS configuration file]]
    - [[#testbed-description][Testbed description]]
    - [[#resources-description][Resources description]]
    - [[#summary][Summary]]
  - [[#deploy-openstack][Deploy OpenStack]]
  - [[#play-with-openstack][Play with OpenStack]]
    - [[#unleash-the-operator-in-you][Unleash the Operator in You]]
- [[#stress-and-visualize-openstack-behavior-using-enos][Stress and Visualize OpenStack Behavior using EnOS]]
  - [[#visualize-openstack-behavior][Visualize OpenStack Behavior]]
  - [[#benchmark-openstack][Benchmark OpenStack]]
  - [[#backup-your-results][Backup your results]]
- [[#add-traffic-shaping][Add Traffic Shaping]]
  - [[#define-network-constraints][Define Network Constraints]]
    - [[#checking-the-constraints][Checking the constraints]]
  - [[#run-dataplane-benchmarks-with-and-without-dvr][Run Dataplane Benchmarks with and without DVR]]
- [[#footnotes][Footnotes]]

* Requirements
To follow the lab you'll need :
- A Web browser (e.g. Firefox [fn:firefox])
- A Grid'5000 account [fn:g5k-account]
- An SSH client (e.g. OpenSSH on Linux/Mac [fn:openssh], Putty on Windows [fn:putty])
  + Check G5K's recommendations to configure SSH for Grid'5000 [fn:g5k-ssh]
  + Check your configuration by typing ~ssh rennes.g5k~ for instance

* Presentation

#+BEGIN_NOTE
Since OpenStack deployment can be quite long (~20, 30 minutes) you might be
interested in starting its deployment before reading the presentation of
OpenStack and EnOS (you can jump to [[sec:enos][this part]] and come back later).
#+END_NOTE

Adrien Lebre gave a lecture regarding Cloud Computing, OpenStack and EnOS. You
can find the slides of this lecture [[http://enos.irisa.fr/tp-polytech/openstack-slides.pdf][here]]. In the following, we quickly present
some information regarding OpenStack, EnOS and the lab we are going to set
today.

** OpenStack
OpenStack is the de-facto solution to manage infrastructures (i.e. compute,
network, storage resources). To that end, it provides management mechanisms as a
modular platform composed of several projects, each of which is in charge of an
aspect of the infrastructure management. Among the various projects (30+), here
is a selection corresponding to the bare necessities to operate infrastructure:
- /nova/: the compute resource manager (i.e. virtual/bare-metal machines and
  containers);
- /glance/: the image store;
- /neutron/: the network manager for compute resources interconnection;
- /keystone/: manage authentication/authorization.

Each project are themselves based on multiple modules. Since OpenStack is
designed as a distributed software, each module can be deployed on different
physical/virtual machines. For instance, here is a set of modules that compose Nova:
- /nova-api/: in charge of managing users' requests;
- /nova-scheduler/: in charge of scheduling compute resources on compute nodes;
- /nova-compute/: in charge of the life-cycle of compute resources.
- ...

To provide all the features expected by an infrastructure manager, OpenStack's
modules need cooperation. For instance, when a user asks /nova/ to boot a VM, the
image is fetched from /glance/, its network interfaces are configured by
/neutron/, supposing /keystone/ authorized the operation. Such cooperation is
possible through three communication channels:
- /REST APIs/: used for inter-project communications;
- /Message queue/ (RabbitMQ): used for intra-project communications;
- /Database/ (MariaDB): used to store project states.

From the user viewpoint, OpenStack can be operated by three ways:
- Horizon: the OpenStack service in charge of providing a Web GUI;
- The OpenStack CLI;
- REST APIs.

** EnOS
EnOS[fn:enos-paper][fn:enos-code] is a holistic framework to conduct evaluations
of different OpenStack configurations in an easy and reproducible manner. In
particular, EnOS helps you in deploying real OpenStack instances on different
types of infrastructure (from virtual environments based on VMs like Vagrant, to
real large-scale testbeds composed of bare-metal machines like Grid'5000),
stressing it and getting feedback.

Many projects exist to deploy OpenStack (e.g.
OpenStack-Ansible[fn:openstack-ansible], OpenStack-Chef[fn:openstack-chef],
OpenStack Kolla[fn:openstack-kolla], Kubernetes[fn:openstack-kubernetes],
Juju[fn:openstack-juju]). EnOS relies on Kolla to deploy OpenStack. Kolla is an
OpenStack project which deploys OpenStack modules as Docker containers, using
Ansible.

EnOS' workflow is the following:
1. EnOS Up: book, provision and bootstrap resources
   + install dependencies (Ansible, Docker)
   + install monitoring tools (cAdvisor, collectd, influxdb, grafana)
2. EnOS Deploy: deploy OpenStack (based on Kolla)
3. EnOS Bench: benchmark OpenStack
4. EnOS Backup: backup the collected metrics
5. EnOS Destroy: release resources

** Topology deployed in this lab 
The lab makes use of EnOS to deploy OpenStack on Grid'5000. In particular, we
will need four G5K machines for our deployment:
1. a machine to run EnOS;
2. a control node: hosts the control modules, projects' APIs and databases;
3. a network node: hosts network agents;
4. a compute node: manages the compute modules where guest VMs live.

The following figure depicts the status of the different components in play
during the lab.

#+BEGIN_SRC
                       +---------------+
+----------------------+ g5k-frontend  +----------------------+
|                      +-------+-------+                      |
|                              |                              |
|                              v                              |
|                      +---------------+                      |
|           +----------+   enos-node   +----------+           |
|           |          +---------------+          |           |
|           |                  |                  |           |
|           v                  v                  v           |
|   +-------+-------+  +-------+-------+  +-------+------ +   |
|   | compute-node  |  | control-node  |  | network-node  |   |
|   |               |  |               |  |               |   |
|   | * container 1 |  | * container 1 |  | * container 1 |   |
|   | * container 2 |  | * container 2 |  | * container 2 |   |
|   | * ...         |  | * ...         |  | * ...         |   |
|   | * container n |  | * container n |  | * container n |   |
|   +---------------+  +---------------+  +---------------+   |
|                                                             |
+-------------------------------------------------------------+
#+END_SRC

#+BEGIN_NOTE
We need a dedicated node to run EnOS because it is discouraged to run
experiments on the frontend. This restriction is meant to avoid disturbing other
users that are logged to the frontend node, since it has limited resources.
#+END_NOTE

* Set the EnOS-node and install EnOS
  <<sec:enos>>

The first step is to determine on which cluster you will deploy OpenStack. To
that end, you can run ~funk~ (Find yoUr Nodes on g5K) from any frontend to see
the availability on G5K:

#+BEGIN_EXAMPLE
user@laptop~$ ssh rennes.g5k
frennes:~$ funk -w 4:00:00
#+END_EXAMPLE

In this example, we check the availability of G5K's clusters for the next four
hours. Check a cluster with at least four nodes available before going further.
Once it is done, reach the cluster's site first, and then, get a new machine
which we will use as our EnOS node (enos-node):
#+BEGIN_EXAMPLE
frennes:~$ ssh nantes
fnantes:~$ oarsub -I -l "nodes=1,walltime=4:00:00" -p "cluster='econome"
#+END_EXAMPLE

In this example, we get a new machine in interactive mode (i.e. "-I") for the
next four hours from the ~econome~ cluster. If it succeeds you should be
directly connected to this node (check your prompt).

#+BEGIN_NOTE
Before going further, you might be interested in using ~tmux~ in order to be
resilient to any failure during your ssh session. Simple run ~tmux~ from the
frontend, then you can reach this /tmux session/ at anytime by running ~tmux a~.
#+END_NOTE

Download EnOS in your working directory, and install it:

#+BEGIN_EXAMPLE
user@enos-node:~$ git clone https://github.com/dpertin/enos.git -b stable/pike
user@enos-node:~$ cd enos
user@enos-node:~/enos$: virtualenv --python=python2 venv
user@enos-node:~/enos$: source venv/bin/activate
(venv) user@enos-node:~/enos$ pip install -e .
#+END_EXAMPLE

#+BEGIN_NOTE
We install EnOS inside a virtualenv to avoid any conflict regarding the version
of its dependencies. Furthermore, it does not install anything outside the
virtual environment which keeps your OS clean. Remember that you have to be in
the virtual environment to use EnOS. If you open a new terminal on ~enos-node~,
type the following:
: user@enos-node:~$ cd ~/enos; source venv/bin/activate;
#+END_NOTE

Check if EnOS works:
#+BEGIN_EXAMPLE
(venv) user@enos-node:~/enos$ enos --help
Enos: Monitor and test your OpenStack.

usage: enos <command> [<args> ...] [-e ENV|--env=ENV]
            [-h|--help] [-v|--version] [-s|--silent|--vv]

General options:
  -e ENV --env=ENV  Path to the environment directory. You should
                    use this option when you want to link to a specific
                    experiment. Not specifying this value will
                    discard the loading of the environment (it
                    makes sense for `up`).
  -h --help         Show this help message.
  -s --silent       Quiet mode.
  -v --version      Show version number.
  -vv               Verbose mode.

Commands:
  new            Print a reservation.yaml example
  up             Get resources and install the docker registry.
  os             Run kolla and install OpenStack.
  init           Initialise OpenStack with the bare necessities.
  bench          Run rally on this OpenStack.
  backup         Backup the environment
  ssh-tunnel     Print configuration for port forwarding with horizon.
  tc             Enforce network constraints
  info           Show information of the actual deployment.
  destroy        Destroy the deployment and optionally the related resources.
  deploy         Shortcut for enos up, then enos os and enos config.
  kolla          Runs arbitrary kolla command on nodes


See 'enos <command> --help' for more information on a specific command.
#+END_EXAMPLE

* Deploy OpenStack using EnOS
** The EnOS configuration file
To deploy OpenStack, EnOS reads a /configuration/ file. This file states the
OpenStack resources you want to measure together with their topology. A
configuration could say, "Deploy a basic OpenStack on a single node", or "Put
OpenStack control services on ClusterA and compute services on ClusterB", but
also "Deploy each OpenStack services on a dedicated node and add WAN network
latency between them". So that EnOS can deploy such OpenStack over your testbed
and run performance analysis.

The description of the configuration is done in a ~reservation.yaml~ file. You
can generate a default one by typing:

: (venv) user@enos-node:~/enos# enos new > reservation.yaml

In the following, we are going to study the different part of this configuration
file.

*** Testbed description
The description of the desired testbed is done in the ~reservation.yaml~ file,
under the ~provider~ key. Way you describe your topology may vary a little bit
depending on the testbed you target. The actual EnOS implementation supports
Vagrant (VBox), Grid’5000, Chameleon and OpenStack itself. To that end, EnOS
define different /providers/ which are in charge of provisioning resources on a
a specific testbed. Please, refer to the EnOS provider
documentation[fn:enos-provider] to find examples of resources description
depending on the testbed.

For the sake of this lab we are going to use the Grid'5000 provider. Use your
favorite text editor to open the ~reservation.yaml~ file, for instance: ~vim
reservation.yaml~, and edit the file so that it fits the example below:

#+ATTR_HTML: style="float:right;margin:0px 0px 20px 20px;"
#+CAPTION: Description the EnOS provider used for our deployment.
#+NAME: lst:topos-g5k
#+BEGIN_SRC yaml
---
# ############################################### #
# Grid'5000 reservation parameters                #
# ############################################### #
provider:
  type: g5k
  name: 'Enos'
  walltime: '02:00:00'
  # mandatory : you need to have exacly one vlan
  vlans:
     nantes: "{type='kavlan'}/vlan=1"
  # Be less strict on node distribution especially
  # when nodes are missing in the reservation
  # or not deployed
  role_distribution: debug
  env_file: /home/discovery/enos_env/debian9-x64-enos-nfs.env
#+END_SRC

In particular, pay attention to 2 elements you might need to adapt here:
1. ~walltime~: define the time of your reservation (2 hours is enough for the
   first part of this workshop, if you plan to stay for the second part you
   should set 4 hours)
2. ~vlans~: to interconnect your resources on G5K, EnOS relies on KaVLAN which
   must be reserved on from the site you plan to deploy OpenStack on, prior the
   experiment. In our example, we reserve a VLAN from the /rennes/ site. Adapt
   the related value regarding the G5K site you plan to use.

#+BEGIN_NOTE
In a classic deployment, all the container images are downloaded and
decompressed by Docker during the deployment process. This has a significant
impact on the time required to deploy OpenStack. In this lab, to boost the
deployment process, we use the G5K environment ~debian9-x64-enos-nfs~, which
contains all resources needed by EnOS to run OpenStack (included all the Docker
container images).
#+END_NOTE

*** Resources description
The description of the desired resources is declared in the ~reservation.yaml~
file, under the ~resources~ key. Here we declare the G5K cluster we target, as
well as the resources we want to deploy:
#+ATTR_HTML: style="float:right;margin:0px 0px 20px 20px;"
#+CAPTION: Description of an EnOS topology for Grid'5000.
#+NAME: lst:topos-g5k
#+BEGIN_SRC yaml
# Resources description
resources:
  econome:
    control: 1
    compute: 1
    network: 1
#+END_SRC

In this example, we declare 3 machines on the ~econome~ cluster (which is
located in rennes): a ~control~ node, a ~compute~ node and a ~network~ node on
which will be deployed all the required OpenStack services as depicted at the
end of the presentation.

*** Summary
At the end, your ~reservation.yaml~ should look like this:

#+INCLUDE: "./reservation.yaml" src yaml

** Deploy OpenStack
EnOS manages all the aspects of an OpenStack deployment by calling ~enos
deploy~. Concretely, the ~deploy~ phase first gets resources on your testbed
following your configuration description. Then, it provisions these resources
with Docker. And finally, it starts each OpenStack services (e.g. Keystone,
Nova, Neutron, ...) inside a dedicated Docker container.

Launch the deployment with:
: (venv) user@enos-node:~/enos$ enos deploy -f reservation.yaml

EnOS is now provisioning three machines on the cluster targeted in the
~reservation.yaml~. Once the machines are provisioned, EnOS will deploy
OpenStack services on them, and you can now display information regarding your
deployment by typing:
: (venv) user@enos-node:~/enos$ enos info

In particular, you should see the IP address of the deployed nodes.

While EnOS deploys OpenStack (it takes ~20,30 minutes), you can observe EnOS
running containers on the control node, for instance. For that, you  one of the deployed node from another terminal of your VM
with:
: user@compute-node:$ sudo docker ps

** Play with OpenStack
The last service deployed is the OpenStack dashboard (Horizon). Once the
deployment process is finished, Horizon is reachable from G5K. More precisely,
Horizon runs in a Docker container on the control node, listening on port 80. To
access Horizon from your own web browser (from your laptop), you need to create
an SSH tunnel from your laptop to the control node, located in G5K. To that end,
your need first to find out the the control node address, and then create the
tunnel. Open a new terminal and type the following:
1. Find the control node address using EnOS:
: (venv) user@enos-node~/enos$ enos info
: (venv) user@enos-node~/enos$ enos info --out json | jq -r '.rsc.control[0].address'
2. Create the tunnel from your laptop:
: user@laptop~$ ssh -NL 8000:<g5k_control_node>:80 <g5k_site>.g5k
: user@laptop~$ ssh -NL 8000:econome-14-kavlan-4.nantes.grid5000.fr:80 nantes.g5k

Once it is done, you can access Horizon from your web browser through
http://localhost:8000 with the following credentials:
- login: ~admin~
- password: ~demo~

From here, you can reach ~Project > Compute > Instances > Launch
Instance~ and boot a virtual machine given the following information:
- a name (e.g., ~horizon-vm~);
- an image (e.g., ~cirros.uec~);
- a flavor to limit the resources of your instance (I recommend
  ~tiny~);
- and a network setting (must be ~private~).

You should select options by clicking on the arrow on the right of
each possibility. When the configuration is OK, the ~Launch Instance~
button should be enabled. After clicking on it, you should see the
instance in the ~Active~ state in less than a minute.

Now, you have several options to connect to your freshly deployed VM.
For instance, by clicking on its name Horizon provides a virtual
console under the tab ~Console~. Use the following credentials to
access the VM:
- login: ~cirros~
- password: ~cubswin:)~

While Horizon is helpful to discover OpenStack features, this is not
how a true operator administrates OpenStack. A true operator prefers
command line interface.

*** Unleash the Operator in You
OpenStack provides a command line interface to operate your Cloud. But
before using it, you need to set first your environment with OpenStack
credentials, so that the command line won't bother you by requiring
credentials each time.

Load the OpenStack credentials:
: (venv) user@enos-node:~/enos$ source current/admin-openrc

You can then check that your environment is correctly set by:
#+BEGIN_EXAMPLE
(venv) user@enos-node:~/enos$ env|grep OS_
OS_PROJECT_DOMAIN_ID=default
OS_USERNAME=admin
OS_IDENTITY_API_VERSION=3
OS_USER_DOMAIN_NAME=default
OS_TENANT_NAME=admin
OS_USER_DOMAIN_ID=default
OS_AUTH_URL=http://10.24.61.255:35357
OS_PROJECT_DOMAIN_NAME=default
OS_REGION_NAME=RegionOne
OS_PROJECT_NAME=admin
OS_PASSWORD=demo
#+END_EXAMPLE

All operations to manage OpenStack are done through one single command
line, called ~openstack~. Doing an ~openstack --help~ displays the
really long list of possibilities provided by this command. Next gives
you a selection of most often used commands to operate your Cloud:
- List OpenStack running services :: ~openstack endpoint list~
- List images :: ~openstack image list~
- List flavors :: ~openstack flavor list~
- List networks :: ~openstack network list~
- List computes :: ~openstack hypervisor list~
- List VMs (running or not) :: ~openstack server list~
- Get details on a specific VM :: ~openstack server show <vm-name>~
- Start a new VM :: ~openstack server create --image <image-name> --flavor <flavor-name> --nic net-id=<net-id> <vm-name>~
- View VMs logs :: ~openstack console log show <vm-name>~

Using all these commands, you can use the CLI to start a new tiny
cirros VM called ~cli-vm~:
#+BEGIN_EXAMPLE
user@enos-node:~/enos$ openstack server create\
  --image cirros.uec\
  --flavor m1.tiny\
  --nic net-id=$(openstack network show private --column id --format value)\
  cli-vm
#+END_EXAMPLE

: (venv) user@enos-node~/enos$ control_node=$(enos info --out json | jq -r '.rsc.control[0].address')
: (venv) user@enos-node~/enos$ network_node=$(enos info --out json | jq -r '.rsc.network[0].address')
: (venv) user@enos-node~/enos$ compute_node=$(enos info --out json | jq -r '.rsc.compute[0].address')

#+BEGIN_NOTE
This exercice has been designed to *run on a cluster where nodes have two
network interfaces*. *If you plan to run the execice on a cluster with a single
network interface, please run the following script on the network node.* You can
check how many network interfaces are associated to a cluster by consulting the
[[https://www.grid5000.fr/mediawiki/images/G5k_cheat_sheet.pdf][Grid'5000 Cheatsheet]]
#+END_NOTE

#+BEGIN_EXAMPLE
#!/usr/bin/env bash

# The network interface
IF=eth0
# This is the list of the vip of eth0
ips=$(ip addr show dev $IF|grep "inet .*/32" | awk '{print $2}')
if [[ ! -z "$ips" ]]
then
  # vip detected
  echo $ips
  docker exec -ti openvswitch_vswitchd ovs-vsctl add-port br-ex $IF && ip addr flush $IF && dhclient -nw br-ex
  for ip in $ips
  do
    ip addr add $ip dev br-ex
  done
else
  echo "nothing to do"
fi
#+END_EXAMPLE

And then display information about your VM with the following command.
: user@enos-node:~/enos$ openstack server show cli-vm
Note in particular the status of your VM. This status will go from
~BUILD~: OpenStack is looking for the best place to boot the VM, to
~ACTIVE~: your VM is running. The status could also be ~ERROR~ if you
are experiencing hard times with your infrastructure.

With the previous ~openstack server create~ command, the VM boots with
a private IP. Private IPs are used for communication between VMs,
meaning you cannot ping your VM from the lab machine. Network lovers
will find a challenge here: try to ping the VM from the lab machine.
For the others, you have to manually affect a floating IP to your
machine if you want it pingable from the lab.
#+BEGIN_EXAMPLE
user@enos-node:~/enos$ openstack server add floating ip\
  cli-vm\
  $(openstack floating ip create public -c floating_ip_address -f value)
#+END_EXAMPLE

Then, ask for the status of your VM and its IPs with:
: user@enos-node:~/enos$ openstack server show cli-vm -c status -c addresses

When the state is ~ACTIVE~ wait one minute or two, the time for the VM
to boot. Then you can ping it on its floating IP and SSH on it:
: user@enos-node:~/enos$ ping <floating-ip> # floating-ip is 10.44.128.0/18
: user@enos-node:~/enos$ ssh -l cirros <floating-ip>

#+BEGIN_NOTE
You can check that the VM finished to boot by looking at its logs with
~openstack console log show cli-vm~. The VM finished to boot when last
lines are:
#+BEGIN_EXAMPLE
=== cirros: current=0.3.4 uptime=16.56 ===
  ____               ____  ____
 / __/ __ ____ ____ / __ \/ __/
/ /__ / // __// __// /_/ /\ \
\___//_//_/  /_/   \____/___/
   http://cirros-cloud.net


login as 'cirros' user. default password: 'cubswin:)'. use 'sudo' for root.
cli-vm login:
#+END_EXAMPLE
#+END_NOTE

Before going to the next section, feel free to play around with the
~openstack~ CLI and Horizon. For instance, list all features offered
by Nova with ~openstack server --help~ and try to figure out how to
SSH on ~cli-vm~ using its name rather than its floating IP.

* Stress and Visualize OpenStack Behavior using EnOS
EnOS not only deploys OpenStack according to your configuration, but
also instruments it with a /monitoring stack/. The monitoring stack
gets performance characteristics of the running services and helps you
to understand the behavior of your OpenStack.

Activating the monitoring stack is as simple as setting the ~enable_monitoring~
to ~yes~ in your ~reservation.yaml~. This key tells EnOS to deploy two
monitoring systems. First, cAdvisor[fn:cadvisor], a tool to collect resource
usage of running containers. Using cAdvisor, EnOS gives information about the
CPU/RAM/Network consumption per cluster/node/service. Second,
Collectd[fn:collectd], a tool to collect performance data of specific
applications. For instance, Collectd enables EnOS to record the number of
updates that have been performed on the Nova database.

The rest of this section, first shows how to visualize cAdvisor and
Collectd information. Then, it presents tools to stress OpenStack in
order to collect interesting information.

** Visualize OpenStack Behavior
A popular tool to visualize information provided by cAdvisor and Collectd (and
whatever monitoring system you could use) is Grafana[fn:grafana]. Grafana is a
Web metrics dashboard. A Docker container is in charge of providing this service
inside the ~control~ node. As a consequence, prior being able to reachable from
your browser, you need to set a tunnel to this service, by typing:
: user@laptop~$ ssh -NL 3000:<g5k_control_node>:3000 <g5k_site>.g5k
: user@laptop~$ ssh -NL 3000:econome-14-kavlan-4.nantes.grid5000.fr:3000 nantes.g5k
You can then access Grafana at http://localhost:3000 with the following
credentials:
- login: ~admin~
- password: ~admin~

The Grafana dashboard is highly customizable. For the sake of simplicity, we
propose to use our configuration file that you can get with:
: user@laptop:~$ curl http://enos.irisa.fr/vagrant-box/grafana_dashboard_rescom2017.json -O

You have then to import this file into Grafana. First, click on the
~Grafana logo > Dashboard > Import > Upload .json file~ and select the
=~/rescom17-enos/grafana_dashboard_rescom2017.json= file. Next, make
names of the right column matching names of the left column by
selecting the good item in the list. And finish by clicking on ~Save &
Open~. This opens the dashboard with several measures on Nova,
Neutron, Keystone, RabbitMQ, ... services.

Keep the dashboard open until the end of the lab, you will see consumption
variation as we will perform stress tests. Eventually, you will see vertical
bars (red, green and blue) crossing your graphs. These bars, called
~annotations~, give specific information regarding the actions triggered by
EnOS.

** Benchmark OpenStack
Stressing a Cloud manager can be done at two levels: at the /control plane/ and
at the /data plane/, and so it is for OpenStack. The control plane stresses
OpenStack API. That is to say, features we used in the previous section to start
a VM, get a floating IP, and all the features listed by ~openstack --help~. The
data plane stresses the usage of resources provided by an OpenStack feature. For
instance, a network data plane testing tool will measure how resources provided
by Neutron handle networks communications.

OpenStack comes with dedicated tools that provide workload to stress
control and data plane. The one for control plane is called
Rally[fn:rally] and the one for data plane is called
Shaker[fn:shaker]. And these two are well integrated into EnOS.

Calling Rally and Shaker from EnOS is done with:
: user@enos-node:~/enos$ enos bench --workload=workload

#+BEGIN_NOTE
At the same time as ~enos bench~ running, keep an eye on the Grafana dashboard
available at http://localhost:3000. At the top left of the page, you can click
on the clock icon ⌚ and tells Grafana to automatically refresh every 5 seconds
and only display the last 5 minutes.
#+END_NOTE

EnOS looks inside ~workload~ directory for a file named ~run.yml~. This file is
the description of the workload to launch. Listing [[lst:run]] shows the definition
of the ~run.yml~ provided in this lab. The [[(rally)][~rally~]] key specifies the list of
[[(scn)][~scenarios~]] to execute (here, only ~boot and list servers~ that asks Nova to
boot VMs and list them) and their customization. The customization could be done
by using the top level [[(top-arg)][~args~]]. In such case, it applies to any scenario. For
instance here, [[(conc)][~concurrency~]] and [[(times)][~times~]] tells Rally to launch ~5~ OpenStack
clients for a total of ~10~ executions of every scenario. The customization
could also be done on a per-scenario basis with the dedicated [[(scn-arg)][~args~]], and thus
could be only applied to a specific scenario. For instance here, the ~30~ value
overrides the ~sla_max_avg_duration~ default value solely in the ~boot and list
servers~ scenario.

#+CAPTION: Description of the workload for this lab.
#+CAPTION: It says to run one Rally scenario that
#+CAPTION: boots and lists VMs.
#+NAME: lst:run
#+BEGIN_SRC yaml -r
---
rally:                                   (ref:rally)
    enabled: yes
    args:                                (ref:top-arg)
      concurrency:                       (ref:conc)
        - 5
      times:                             (ref:times)
        - 10
    scenarios:                           (ref:scn)
      - name: boot and list servers
        file: nova-boot-list-cc.yml
        args:                            (ref:scn-arg)
          sla_max_avg_duration: 30
shaker:
  enabled: yes                            (ref:disabled)
  scenarios:
    - name: OpenStack L3 East-West UDP
      file: openstack/udp_l3_east_west
#+END_SRC

Rally and Shaker provide a huge list of scenarios on their respective
GitHub[fn:rally-scenarios][fn:shaker-scenarios]. Before going further, go
through the Rally list and try to add the scenario of your choice into the
~run.yml~. Note that you have to download the scenario file in the ~workload~
directory and then put a new item under the [[(scn)][~scenarios~]] key. The new item should
contain, at least, the ~name~ of the scenario and its ~file~ path (relative to
the ~workload~ directory).

** Backup your results
Rally and Shaker produce reports on the executed scenarios. For instance, Rally
produces a report with the full duration, load mean duration, number of
iterations and percent of failures, per scenario. These reports, plus data
measured by cAdvisor and Collectd, plus logs of every OpenStack services can be
stored in a backup archive by EnOS with:
: user@enos-node:~/enos$ enos backup

The argument ~backup_dir~ tells where to store the backup archives. If you look
into this directory, you will see, among others, an archive named
~enos-node-rally.tar.gz~. Concretely, this archive contains a backup of Rally
database with all raw data and the Rally reports. You can extract the rally
report with the following command and then open it in your favorite browser:
: host:~/enos-node$ tar -x root/rally_home/report.html -f enos-node-rally.tar.gz
: host:~/enos-node$ firefox root/rally_home/report.html

If you look carefully, you will see that execution of Nova boot and
list fails because of a SLA violation. You can try to customize
listing [[lst:run]] to make the test pass.

* Add Traffic Shaping
EnOS allows to enforce network emulation in terms of latency, bandwidth
limitation and packet loss.

** Define Network Constraints
Network constraints (latency/bandwidth limitations) are enabled by the use of
groups of nodes. Resources must be described using a ~topology~ description
instead of a ~resources~ description. For instance, listings [[lst:topos-g5k]]
defines three groups named grp1, grp2 and grp3.

#+ATTR_HTML: style="float:right;margin:0px 0px 20px 20px;"
#+CAPTION: Description of a topology for Grid'5000.
#+NAME: lst:topos-g5k
#+BEGIN_SRC yaml
topology:
  grp1:
    econome:
      control: 1
      network: 1
  grp[2-3]:
    econome:
      compute: 1
#+END_SRC

Constraints are then described under the ~network_constraints~ key in
the ~reservation.yaml~ file:
#+BEGIN_SRC yaml
network_constraints:
  enable: true
  default_delay: 25ms
  default_rate: 100mbit
  default_loss: 0.1%
  constraints:
    - src: grp1
      dst: grp[2-3]
      delay: 50ms
      rate: 1gbit
      loss: 0%
      symetric: true
#+END_SRC

And enforce these constraints with ~enos tc~, which results in:
- Network delay between machines of ~grp1~ and the machines of the
  other groups is 100ms (2x50ms: symmetric).
- Bandwidth between machines of ~grp1~ and the machines of the other
  groups is 1 Gbit/s.
- Packet loss percentage between machines of ~grp1~ and the machines
  of the other groups is 0%.
- Network delay between machines of ~grp2~ and ~grp3~ is 50ms.
- Bandwidth between machines of ~grp2~ and ~grp3~ is 100Mbit/s.
- Packet loss percentage between machines of ~grp2~ and ~grp3~ is
  0.1%.

#+BEGIN_NOTE
To call ~enos tc~, resources must be available, thus ~enos deploy~
must have been called before.
#+END_NOTE

*** Checking the constraints
Invoking ~enos tc --test~ generates various reports to validate the
constraints. They are based on ~fping~ and ~flent~ latency and
bandwidth measurements respectively. The report is located in the
result directory.

** Run Dataplane Benchmarks with and without DVR
Run a first time the Shaker ~dense_l3_east_west~ scenario. In this
scenario Shaker launches pairs of instances on the same compute node.
Instances are connected to different tenant networks connected to one
router. The traffic goes from one network to the other (L3 east-west).
Get the Shaker report with ~enos backup~ and analyze it. You will
remark that network communications between two VMs co-located on the
same compute are 100ms RTT. This is because packet are routed by
Neutron service that is inside ~grp1~ and VMs are inside the ~grp2~
(or ~grp3~).

Now, reconfigure Neutron to use DVR[fn:dvr]. DVR will push Neutron
agent directly on compute of ~grp2~ and ~grp3~. With EnOS, you should
do so by updating the ~reservation.yaml~ and add ~enable_dvr: "yes"~
under the ~kolla~ key. Then call the following line to tell EnOS to
reconfigure Neutron:
: enos os --tags=neutron --reconfigure

Finally, re-execute the ~dense_l3_east_west~ scenario and compare your
result with previous one. You will see that you no more pay the cost
of WAN latency.

This experiment shows the importance of activating DVR in a WAN
context, and how you can easily show that using EnOS. Do not hesitate
to take a look at the complete list of Shaker scenarios on their
GitHub[fn:shaker-scenarios] and continue to have fun with EnOS.

* Footnotes

[fn:openstack] https://www.openstack.org/
[fn:kolla-ansible] https://docs.openstack.org/developer/kolla-ansible/
[fn:enos-paper] https://hal.inria.fr/hal-01415522v2
[fn:enos-code] https://github.com/BeyondTheClouds/enos
[fn:virtualbox-downloads] https://www.virtualbox.org/wiki/Downloads
[fn:vagrant] https://www.vagrantup.com/
[fn:vagrant-downloads] https://www.vagrantup.com/downloads.html
[fn:g5k] https://www.grid5000.fr/
[fn:chameleon] https://www.chameleoncloud.org/
[fn:openstack-ansible] https://github.com/openstack/openstack-ansible
[fn:openstack-chef] https://github.com/openstack/openstack-chef-repo
[fn:openstack-kolla] https://wiki.openstack.org/wiki/Kolla
[fn:openstack-kubernetes] https://github.com/stackanetes/stackanetes
[fn:openstack-juju] https://jujucharms.com/openstack
[fn:enos-box] http://enos.irisa.fr/vagrant-box/rescom17.box
[fn:enos-provider] https://enos.readthedocs.io/en/latest/provider.html
[fn:enos-g5k-provider] https://enos.readthedocs.io/en/latest/provider/grid5000.html
[fn:enos-vagrant-provider] https://enos.readthedocs.io/en/latest/provider/vagrant.html
[fn:vagrantfile] https://www.vagrantup.com/docs/vagrantfile/index.html
[fn:cadvisor] https://github.com/google/cadvisor
[fn:collectd] https://collectd.org/
[fn:grafana] https://grafana.com/
[fn:rally] https://rally.readthedocs.io/en/latest/
[fn:shaker] https://pyshaker.readthedocs.io/en/latest/
[fn:rally-scenarios] https://github.com/openstack/rally/tree/master/rally/plugins/openstack/scenarios
[fn:shaker-scenarios] https://github.com/openstack/shaker/tree/master/shaker/scenarios/openstack
[fn:dvr] https://wiki.openstack.org/wiki/Neutron/DVR
[fn:firefox] https://www.mozilla.org/fr/firefox/
[fn:g5k-account] https://www.grid5000.fr/mediawiki/index.php/Grid5000:Get_an_account
[fn:openssh] https://www.openssh.com/portable.html
[fn:putty] https://www.putty.org/
[fn:g5k-ssh] https://www.grid5000.fr/mediawiki/index.php/SSH#Setting_up_a_user_config_file

* COMMENT Local variables
See http://www.i3s.unice.fr/~malapert/org/tips/emacs_orgmode.html#orgheadline32
See https://orgmode.org/worg/org-tutorials/org-publish-html-tutorial.html#org41273ed

# Local Variables:
# org-html-postamble: "<p class=\"author\">Authors: %a</p>
# <p class=\"email\">Email: %e</p>
# <p class=\"github\">Find a typo, wanna make a proposition:
#  <a href=\"https://github.com/BeyondTheClouds/enos-scenarios/issues/new?title=tp-g5k-fit\">open an issue</a></p>
# <p class=\"date\">Last modification: %C</p>
# <p class=\"license\">This work is licensed under a
#   <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">
#     Creative Commons Attribution-ShareAlike 4.0 International License
#   </a>.
# </p>
# <p class=\"creator\">%c – theme by
#  <a href=\"http://gongzhitaao.org/orgcss\">http://gongzhitaao.org/orgcss</a></p>"
# End:
